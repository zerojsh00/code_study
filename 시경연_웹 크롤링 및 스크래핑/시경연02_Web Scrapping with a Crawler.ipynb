{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) 정적 스크래핑 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스크래핑 예제 자료 URL : \n",
    "\n",
    "\n",
    ">- <b><i>文대통령 최저임금 사과에…與 반응 자제, 한국당 \"소주성 오기\"</i></b>\n",
    "- https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=100&sid2=269&oid=001&aid=0010952347"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = 'https://news.naver.com'\n",
    "url_sub = '/main/read.nhn?mode=LS2D&mid=shm&sid1=100&sid2=269&oid=001&aid=0010952347'\n",
    "url = url_base + url_sub\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urlopen 함수의 파라미터로 url의 문자열을 넣어주고, 리턴값으로 HTTPResponse 객체를 받는다.\n",
    "html = urlopen(url)\n",
    "print(html)\n",
    "print(type(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTPResponse 객체를 html의 파라미터(매개변수)로 넣어주어 BeautifulSoup의 객체를 활용할 수 있다!\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(soup.prettify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (tip) 크롬 개발자도구 (ctrl + shift + i)로 제목 부분 copy selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 제목 긁어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.select_one('#articleTitle'))\n",
    "print(type(soup.select_one('#articleTitle')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = soup.select_one('#articleTitle').string\n",
    "title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 본문 긁어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select_one('#articleBodyContents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   위와 같이 무언가 정신없이 원하지 않는 태그들이 섞여있다. 어떻게 본문 내용만 parsing 해볼까? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select_one('#articleBodyContents > script')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in soup.select_one('#articleBodyContents > script').next_siblings:\n",
    "    print(type(each))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in soup.select_one('#articleBodyContents > script').next_siblings:\n",
    "    if type(each) == bs4.element.Tag:\n",
    "        print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "for each in soup.select_one('#articleBodyContents > script').next_siblings:\n",
    "    if type(each) != bs4.element.Tag:\n",
    "        print(each.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 오케이 ! 방법을 대충 알았으니, 지금까지의 과정을 나중에도 쉽게 사용할 수 있도록 필요한 부분만 함수화 해보자 !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "# URL만 파라미터로 넣어주면 제목과 본문을 뽑아주는 함수 \n",
    "def scrap(url) :\n",
    "    # urlopen 함수의 파라미터로 url의 문자열을 넣어주고, 리턴값으로 HTTPResponse 객체를 받는다.\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "\n",
    "    # 제목 가져오기\n",
    "    title = soup.select_one('#articleTitle').string\n",
    "    \n",
    "    # 본문 가져오기\n",
    "    text = ''\n",
    "    for each in soup.select_one('#articleBodyContents > script').next_siblings:\n",
    "        if type(each) != bs4.element.Tag:\n",
    "            text += each.strip()\n",
    "\n",
    "    # Return\n",
    "    return title, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "title, text = scrap(url)\n",
    "print(title)\n",
    "print()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = 'https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=100&sid2=269&oid=437&aid=0000214928'\n",
    "\n",
    "# test\n",
    "title, text = scrap(url1)\n",
    "print(title)\n",
    "print()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Crawler 개발 : 동시에 여러 기사를 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 페이지의 '제목 링크'에 해당하는 a태그는 ? 한 페이지 내 모든 기사들의 링크 주소를 먼저 파악해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_url = 'https://news.naver.com/main/list.nhn?mode=LS2D&sid2=269&sid1=100&mid=shm&date=20190714&page=1'\n",
    "html = urlopen(page_url)\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select('#main_content > div.list_body.newsflash_body > ul.type06_headline > li > dl > dt:nth-child(2) > a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "title_list = soup.select('#main_content > div.list_body.newsflash_body > ul.type06_headline > li > dl > dt:nth-child(1) > a')\n",
    "title_url_list = []\n",
    "for each_title in title_list :\n",
    "    print(each_title['href'])\n",
    "    title_url_list.append(each_title['href'])\n",
    "    print('title_url_list에 \"{}\" 추가 완료'.format(each_title['href']))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 한페이지 내 모든 기사들의 링크들을 긁어왔다!\n",
    "title_url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자, 이것도 또 쓸거니깐 함수화 해보자 !\n",
    "\n",
    "def get_urls_in_page(page_url) :\n",
    "    \n",
    "    html = urlopen(page_url)\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    title_list = soup.select('#main_content > div.list_body.newsflash_body > ul.type06_headline > li > dl > dt:nth-child(1) > a')\n",
    "    title_url_list = []\n",
    "    for each_title in title_list :\n",
    "#         print(each_title['href'])\n",
    "        title_url_list.append(each_title['href'])\n",
    "#         print('title_url_list에 \"{}\" 추가 완료'.format(each_title['href']))\n",
    "#         print()\n",
    "    \n",
    "    return title_url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_urls_in_page('https://news.naver.com/main/list.nhn?mode=LS2D&sid2=269&sid1=100&mid=shm&date=20190714&page=1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 여러 페이지에 접근해보자 ! 각 페이지를 클릭했을 때 주소의 변화를 살펴보면 ...?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://news.naver.com/main/list.nhn?mode=LS2D&sid2=269&sid1=100&mid=shm&date=20190714&page=1\n",
    "- https://news.naver.com/main/list.nhn?mode=LS2D&sid2=269&sid1=100&mid=shm&date=20190714&page=2\n",
    "- https://news.naver.com/main/list.nhn?mode=LS2D&sid2=269&sid1=100&mid=shm&date=20190714&page=3\n",
    "\n",
    "...\n",
    "\n",
    "끝에 <i>page=숫자</i> 부분만 바뀌고있네 !! 바뀌고 있는 숫자들을 변수로 표현해서 반복문 돌리면 되지 않을까?\n",
    "\n",
    "연습으로 20페이지까지만 싹 스크랩해보자 !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall : range\n",
    "for x in range(1,21):\n",
    "    print(x, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 마지막 'page=숫자' 부분만 뺀 url\n",
    "page_url_base = 'https://news.naver.com/main/list.nhn?mode=LS2D&sid2=269&sid1=100&mid=shm&date=20190714&page='\n",
    "\n",
    "page_url_list = []\n",
    "\n",
    "for x in range(1,21) :\n",
    "    page_url_list.append(page_url_base + str(x))\n",
    "\n",
    "page_url_list # 얘를 반복해서 모든 기사들의 URL을 한번에 긁어올 수 있겠다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모든 기사의 URL을 한번에 !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서서 다음의 함수들을 정의했다.\n",
    "\n",
    "1. <i><b>scrap(url)</b></i> : URL만 파라미터로 넣어주면 기사 내 제목과 본문을 뽑아주는 함수\n",
    "2. <i><b>get_urls_in_page(page_url)</b></i> : page URL넣어주면 해당 페이지에 있는 기사들의 모든 링크 주소를 뽑아주는 함수 \n",
    "\n",
    "그리고 page_url_list에는 각 페이지의 주소들이 들어있다. 크롤러를 활용하여 반복문을 통해서 모든 기사들의 제목과 내용을 스크랩 해보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 진행 정도를 그래프로 그려주는 기능\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_title = []\n",
    "all_text = []\n",
    "\n",
    "for page_url in tqdm(page_url_list) :\n",
    "    \n",
    "    # 각 페이지 내 모든 기사 링크 주소들을 리스트로 받음\n",
    "    page_url_list_ = get_urls_in_page(page_url)\n",
    "    \n",
    "    # 각 페이지 내 모든 기사 링크 주소들에 접근하여 제목과 본문을 뽑아줌\n",
    "    for url in page_url_list_ :\n",
    "        title, text = scrap(url)\n",
    "        \n",
    "        all_title.append(title)\n",
    "        all_text.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_text[13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) pandas를 이용하여 excel로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'제목':all_title, '본문내용':all_text})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('./네이버 뉴스기사 스크랩.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
